= AI創薬のための ケモインフォマティクス入門
@fmkz___, @iwatobipen
v0.10014(Draft) 2019/03/08
:toc:
:toc-title: 目次
:lang: ja
:doctype: book
:docname: AI創薬のためのケモインフォマティクス入門
:imagesdir: ./images
:pdf-fontsdir: fonts
:pdf-style: py4c-theme.yml
:source-highlighter: coderay
:title-logo-image: image::souyakuchan.png[mishima.syk]
== 1章: はじめに
:imagesdir: ./images

ケモインフォマティクス(Chemoinformatics)とは、主に化学に関連するデータをコンピュータを用いて解析し、様々な課題を解くために用いる方法論のことです。ケモインフォマティクスという言葉は1990年終わりから2000年はじめに定義され、現在では製薬業界でも薬学系のアカデミアで一般的に利用されている技術となっています。

ケモインフォマティクスは薬剤の効果と化合物の形状との関連性を解析したり、大量の化合物情報の可視化、化合物の類似性に基づいたクラスタリングなどの多岐にわたるプロセスで活用されていますが、近年ではディープラーニング(Deep Learning)の創薬応用がライフサイエンス分野でのトレンドとなっています。特に創薬化学の領域では、活性や物性を予測するQSAR（構造活性相関）への適用だけではなく、**新規デザインへの応用**や**合成経路提案**への適用といった従来のケモインフォマティクスではあまり扱わなかった領域においても応用研究が盛んに行われています。

.化合物デザインはイノベーティブ
****
そもそも**どのような化合物を作るべきか?、またそれをどのように合成するか?**を考えるプロセスは背景知識と想像力が求められる領域であり、従来は人以外が担うのは難しい領域であると認識されていましたが、そういう領域に対してもAIと呼ばれるものの進出がここ数年(2017-2019)で急速に進みました。
****

ケモインフォマティクスはすでに様々な場面で利用されていますが関連情報はあまりありませんでした。この理由として考えられることはいくつかありますが、オープンソースのツールキットがずっと存在しなかったこととオープンなデータベースが存在しなかったという二点が最も大きな原因なのは間違いありません。しかし、RDKitというオープンソースのケモインフォマティクスツールキットとChEMBLという公開データベースの登場により、この点は急速に解消しました。

.化合物関連データベースの歴史
****
そもそも公共の化合物関連のデータベースというものは歴史が浅いです。化合物関連データベース自体の歴史が浅いのかと言われれば全然そんなことはなく、実は昔から色々なデータベースが存在しましたがほぼ全てが商用のサービスでした。オープンな化合物データベースが登場したのはごく最近のことです。アミノ酸、塩基配列、蛋白質結晶構造などのデータベースはかなり以前から無償で提供されていたのとは全く異なる状況だったと言えます。公開データベースがないという状況はオープンネスを失わせる要因の一つでした。
****

近年では、ケモインフォマティクスもバイオインフォマティクス同様、ウェブで検索すれば多くの情報がすぐに得られ、自己学習することは十分可能となっています。

このような状況ではありますが、それを初学者に強いるのはなかなかつらいことだと思い、最初の一歩を踏み出すためのまとまった情報として、「ケモインフォマティクスに関する基礎を学び、それらを応用できるようになるようなコンテンツ」を用意することにしました。近年のAI創薬ブームを考慮して後半の章では「AI創薬」の文脈で用いられるディープラーニングを利用した化合物の活性予測と化合物提案の章を追加していますので、一通り学習することで、最近のトレンドについていけるようにはなっていると思います。

=== RDKitとは

warning:: ここは@iwatobipenがRDKitについて熱く語るサブセクションです。下書き段階での「申し上げる」とか「踏まえて」等の言い回しをそのまま生かし、自称が「拙者」で「ござる」調が＠iwatobipen風文体です。

拙者、本書の一部を執筆する＠iwatobipenと申す。ここではRDKitに関して熱く語ってみようと思うでござる。

RDKitのRDはなんなのか？実は**Rational Discovery**の略称であり現在のオープンソースの前身となるフレームワークは2000年に開発されたでござる。随分と古いでござるね。その後、2006年にコードがオープンソースになりsourceforgeから公開されたでござる。PythonのケモインフォマティクスツールキットはRDKit以外にOpenBabelもあるぞと思われる読者もござろう。OpenBabelは2005年に最初のリリースがなされておる。いずれも、もう10年以上の歴史があるツールキットでござる。拙者がこの辺りに興味を持ち始めた2012年ころはどちらかというとOpenBabelの方がメジャーだったように記憶しておる。当時、日本語の記事はほぼ皆無であり、拙者は本書共著者であり業界のパイオニアでもある@fmkz___殿のlink:https://kzfm.hatenablog.com/archive[ケモインフォクックブック]などを参考にRDKitのコードを書いて試行錯誤していたでござるよ。なお、ケモインフォ関連のヒストリを追いたい御仁はこちらのlink:http://blog.kzfmix.com/entry/1542711744[記事]を一読されるとよかろう。

おっと話が横道に逸れてしまった。本題に戻ろう。

開発者のGreg Landorum氏いわく

[quote, Greg Landorum]
RDKitはケモインフォマティクスにおけるSwiss Army Knifeであり、様々な機能ピースの集合体である

これはまさに的を得た表現でござる。link:https://www.rdkit.org/docs/[公式ドキュメント]を見ればわかるでござろうが、既に色々な機能が用意されておるのだ。
化合物情報の読み込み、書き込みに始まり、構造の描画、３次元構造配座発生、Rグループ分解、記述子、フィンガープリント計算、ファーマコフォア算出などなど、挙げればきりがないほどの機能が実装されておる。解析から可視化まで幅広い範囲をカバーできるのだ。
さらにContributerらがRDKitを利用して開発したツール群がその熱い想いとともにlink:https://github.com/rdkit/rdkit/tree/master/Contrib[Contrib]フォルダーに詰められておるのだ。どうじゃ使ってみたくならんか？。拙者はもう書きながらも早くRDKitに触りたくなってきたでござる。

NOTE: @iwatobipenももちろんContributerの1人で、link:https://github.com/rdkit/rdkit/tree/master/Contrib/Fastcluster[Fastcluster]という大量の化合物ライブラリを高速にクラスタリングするコードを提供しています。(by @fmkz___)

RDKitは開発やユーザーコミュニティの活動も活発で、どんどん機能追加がされておる。世界中の有能な研究者が全体で盛り上げ開発していくスタイルはオープンソースの強みであり、魅力であろう。もしチャンスがあれば毎年開催されるRDKit User Group Meetingへの参加を検討するのもよかろう。Face2Faceでユーザー同士議論ができるのは何事にも代え難いものがあるでござる。
また、先ほど拙者が使い始めた当時は日本語の情報ほぼ皆無であったと申したが、近年は非常に良質な日本語記事もたくさん増えておる。下記に何個か例を挙げたでござる。Qiitaにも多くの記事が掲載されているでござるよ。

また、有志によるlink:http://rdkit-users.jp/[RDKit-users-jp]も立ち上がっておる。英語での質問がちょっと、、、と思われる御仁はこちらに質問を投げかけるとよかろう。また、最新版のRDKitのリポジトリには日本語のドキュメントもマージされておる。こちらも参考になるであろう。
本書ではRDKitの一部の機能しか使わん。それでも非常に多くのことができると感じていただけるはずじゃ。興味持ちはじめの一歩を踏み出したら後はどんどん自分の興味、意欲のままに足を進めていけばよかろう。何かわからないことがあれば上記のコミュニティに問いかけ、本書のリポジトリへIssueとして投稿してみるのもよかろう。
**さあそれでは始めよう！**

==== 主な日本語解説サイト

- link:http://rdkit-users.jp/[rdkit-users.jp]
- link:https://magattaca.github.io/RDKit_unofficial_translation_JP/[RDKitドキュメンテーション非公式日本語版サイト]
- link:https://future-chem.com/[化学の新しいカタチ]

=== 対象読者

次のような方々を読者として想定しています。

- 医学薬学系の大学院生及び薬学系のデータ解析を行いたいポスドク
- 製薬企業の薬理研究者で自分のデータを自分で解析したい人
- 創薬化学者でケモインフォマティクスの必要性を感じている方
- 突然企業でケモインフォマティクス要員にアサインされた方
- AI創薬に興味があるがなにからはじめたらいいかわからない人

=== 本書のコードについて

本書で使用したプログラミングコードは全てlink:https://github.com/Mishima-syk/py4chemoinformatics[Mishima.sykのpy4chemoinformaticsリポジトリ]のnotebooksディレクトリに置いてありますので利用してください。またそれぞれの章の最初のimage:jupyter.png[width="20"]にその章のJupyter notebooへのリンクを張っていますので適宜参照してください。

2章のインストールを行うとgitコマンドがつかえるようになっていますので以下のコマンドでpdfを含む全てのデータがダウンロードされます。

[source, bash]
----
$ git clone https://github.com/Mishima-syk/py4chemoinformatics.git
----


=== おまけ

.Chemoinformatics or Cheminformatics?
****
もともとはBioに対してChemoと語感を合わせて登場してきたように記憶しているが、link:https://jcheminf.biomedcentral.com/[Journal of Cheminformatics]の創刊により一時期Chemに大きく離されていました。

最近のlink:https://trends.google.co.jp/trends/explore?date=all&q=chemoinformatics,cheminformatics[Google trend]によるとどちらでもいいようですが個人的にはRhymeを重視したほうが良いと思うので本書ではChemoの方を使うことにします。
****

=== 謝辞

本書を執筆するにあたり、バグフィックスや改善のための助言をしてくれた以下の方々に感謝いたします。

link:https://twitter.com/ReLuTropy[@ReLuTropy],
link:https://twitter.com/ski_nanko[@ski_nanko],
link:https://twitter.com/torusengoku[@torusengoku],
link:https://twitter.com/yamasaKit_[@yamasaKit_]


ここから先は(Nujabes - reflection eternalを聴きながら書きました　by @fmkz___ 2019/03/03)

まず、本書を書くきっかけとなったlink:https://twitter.com/bonohu[@bonohu]に感謝したいと思います。@bonohuのlink:https://www.amazon.co.jp/dp/4895929019[Dr. Bonoの生命科学データ解析]の出版後のMishima.sykのミーティングで「Bono本のChemoinformatics版あったらいいよね」という話がどこからともなくでた際に、「書けばええんちゃう、むしろなんで書かんの？」と言ってくれたことが本書を執筆するきっかけであることは間違いありません。またlink:https://twitter.com/souyakuchan[@souyakuchan]のlink:https://adventar.org/calendars/3041[創薬 Advent Calendar 2018]も執筆のいい刺激になりました。というより、ここで章立てしなかったら具体的に動き出さなかったと思います。

また、忘れてはいけないのはy-samaの存在です。link:http://mishima-syk.github.io/[Mishima.syk]を初期から盛り上げてきたy-samaは2019/01/06に永眠しました。彼はlink:https://qiita.com/y\__sama/items/5b62d31cb7e6ed50f02c[データサイエンティストを目指す人のpython環境構築 2016]やlink:https://medium.com/@y__sama/druglikeness%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E3%82%88%E3%82%82%E3%82%84%E3%81%BE%E8%A9%B1-8310cec5ffc6[Druglikenessについてのよもやま話]といった素晴らしいエントリを残しました。彼が存命であればきっと3人で執筆していたし、内容ももっと充実していたことでしょう。この出来事も我々に執筆しようという強い動機を与えました。

最後にMishima.sykに参加して美味しいワインやビールを飲みなから毎度熱い議論を交わしていただいた参加者の方々にも感謝します。いくつかのコンテンツはMishima.sykでの発表をもとにしており、みなさんのフィードバックをもとに加筆訂正してあります。

もし、本書を読んで、ケモインフォマティクスって面白いなと感じたり、創薬やってみたいなと感じる方がいたら、是非Mishima.sykに参加してみてください。きっと楽しいと思います。今後の創薬研究では所属を超えてお互いにプッシュしあって自身のスキルを高めていくことが重要になるでしょう。というより、既にそういう社会になっているのだと思います。本書が皆さんの楽しい研究生活を送る役に立てば幸いです。

[quote, y__sama]
やりたいことをやって生きてきて 私自身は自分の人生に後悔はありません
人生は楽しんだもの勝ち
皆さんも嫌なことは嫌だと言って自分の喜びを最大限に追い求めて人生を満喫した方が楽しいと思いますよ
皆様の人生に幸多い事を願っています

=== License

This document is copyright (C) 2019 by @fmkz___ and @iwatobipen

This document is link:https://github.com/Mishima-syk/py4chemoinformatics/blob/master/LICENSE[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
Public License].

image::by-nc-sa.png[CC-BY-NC-SA, width=100]

<<<
== 2章: ケモインフォマティクスのための環境を整えよう

本書を読み進めるにあたり必要な環境構築を行いましょう。
本書の実行環境はPython3.6を想定しています。

=== Anacondaとは

Anacondaとは、機械学習を行うための準備を限りなく楽にするためのパッケージで、これを利用することで、機械学習をすぐに始められるようになります。

==== Q&A

なぜAnacondaを利用するのか？::
  機械学習で広く用いられているプログラミング言語Pythonは多くの有用なライブラリが用意されており、言語自体も非常にわかりやすく学習コストが低いために、ケモインフォマティクスに限らず多くの場面で使われています。しかしながら、それらのライブラリの多くは標準のPythonに付属しておらず自分でインストールする必要があります。プログラミングに精通している方にとっては大した問題ではありませんが、初学者や単純に機械学習をするためPythonを導入したい方には環境構築は大変苦痛な作業です。Anacondaを利用することで、このプログラミング環境の構築作業の手間を大幅に削減することができます。
Pythonには大きく2.x系のバージョンと3.x系のバージョンとがありますが?::
  link:https://pythonclock.org/[2.x系はサポートが2020年までとなっている]のであえて新規にPythonを学ぶ方は2.x系を使う必要は必要性はありません。

=== Anacondaのインストール方法

では早速Anacondaをインストールしましょう。link:https://www.anaconda.com/[公式サイト]にアクセスし、ご自身の環境にあったインストーラー(Python3.7version(201901現在))をダウンロードします。OSがLinuxであればターミナルからインストーラーを実行します。

[source, bash]
----
$ bash ~/Downloads/Anaconda3-4.1.0-Linux-x86_64.sh
----

インストーラーが起動しいくつかの質問をされますが、基本的にエンターまたはYesで進めてください。

Anacondaのインストールが完了すると、コマンドプロンプトまたはターミナルから'conda'コマンドが使えるようになるはずです。

=== Packageのインストール

今回はPython3.6の環境を構築し作業を進めていくので次の手順で開発環境を構築します。コマンドの-nの後ろは"py4chemoinformatics"としていますが皆さんの好きな名前でも構いません。
環境構築後、本章以降で利用するパッケージをインストールします。

[source, bash]
----
$ conda create -n py4chemoinformatics python3.6
$ source activate py4chemoinformatis

# install packages
$ conda install -c conda-forge rdkit
$ conda install -c conda-forge seaborn
$ conda install -c conda-forge ggplot
$ conda install -c conda-forge git
----

=== インストールしたパッケージの説明

==== RDKit

RDKitはケモインフォマティクスの分野で最近よく用いられるツールキットの一つです。オープンソースソフトウェア(OSS)と呼ばれるものの一つで、無償で利用することが可能です。詳しくはlink:ch01_introduction.asciidoc[はじめに]を参照してください。

==== seaborn

link:https://seaborn.pydata.org/[統計データの視覚化のためのパッケージ]の一つです。

==== ggplot

グラフ描画パッケージの一つで**一貫性のある文法で合理的に描ける**ことが特徴です。もともとはRという統計解析言語のために開発されましたが、yhatという会社によりlink:http://ggplot.yhathq.com/[Pythonに移植]されました。

==== Git

バージョン管理システムです。本書ではGitについては説明しませんのでもしGitについて全然知らないという方はlink:https://backlog.com/ja/git-tutorial/[サルでもわかるGit入門]でも読みましょう。

「はじめに」でも説明しましたが、以下のコマンドでpdfを含む全てのデータがダウンロードされますので必要に応じてダウンロードしてください。

[source, bash]
----
$ git clone https://github.com/Mishima-syk/py4chemoinformatics.git
----

=== Condaについてもう少し詳しく

先に説明したAnacondaでインストールされるデフォルトのPythonのージョンは3.7ですが、本書執筆時点で配布されている最新のRDKitはPython3.6を必要とします。このためcondaで仮想環境を構築し、必要なバージョンのPythonをインストールしました。

なぜ仮想環境を作るのでしょうか::
 いくつかのシステムでは様々な機能を提供するために内部的にPythonを利用しているため、特定のパッケージのためにPythonのバージョンを変更してしまうと問題が起こることがあります。仮想環境はこのような問題を解決します。もし、パッケージが異なるライブラリのバージョンを要求しても仮想的なPython環境を準備して試行錯誤できます。不要になれば仮想環境を簡単に削除でき、もとの環境にトラブルを持ちこむこともありません。このように、ひとつのシステム内にそれぞれ個別の開発環境を作成できるようにすることで開発時によく起こるライブラリの依存問題やPythonのバージョンの違いに悩まされることがなくなります。

本書では本書用に一つだけ仮想環境を用意しますが、実際はいくつもの仮想環境をつくって開発することが多いです。そのため、よく利用するcondaのサブコマンドを挙げておきます。

[source, bash]
----
$ conda install <package name>　# install package
$ conda create -n 仮想環境の名前 python=バージョン　# 仮想環境の作成。
$ conda info -e # 作った仮想環境一覧の表示
$ conda remove -n 仮想環境の名前 # 仮想環境の削除
$ source activate 仮想環境の名前 # 仮想環境を使う(mac/linux)
$ activate 仮想環境の名前 # 仮想環境を使う(Windows)
$ source deactivate # 仮想環境から出る
$ conda list # 今使っている仮想環境にインストールされているライブラリの一覧を表示
----

<<<
== 3章: Pythonプログラミングの基礎

=== Pythonの基礎

この章ではPythonに触れたことのない読者のために**効率的に勉強するため**のサイトや本などを紹介します。
もしこれ以降の章でわからないことなどがあったら、この章のサイトや本を参考に学んでみてください。

==== Pythonを本で学びたい

https://www.amazon.co.jp/dp/4774196436/[Pythonスタートブック増補改訂版]::
プログラミング自体が初心者であればこの本が良いでしょう。

https://www.amazon.co.jp/dp/B01NCOIC2P/[みんなのPython 第4版]::
JavascriptやJavaなどのなにかプログラミングを少しかじっていて、これからPythonを覚えたいのであればこちらの本をおすすめします。

==== Pythonを本以外で学びたい

https://www.pycon.jp/support/bootcamp.html[Python Boot Camp(初心者向けPythonチュートリアル)]::
一般社団法人PyCon JPが開催している初心者向けPythonチュートリアルイベントです。全国各地で行われているので近くで開催される場合には参加するとよいでしょう

https://connpass.com/category/Python/[その他ローカルコミュニティなど]::
あちこちで入門者向けからガチのヒト向けまでの勉強会やコミュニティなどもあるので、そういうのに参加してモチベーションを高めるのもよい方法です。

https://www.udemy.com/topic/python/[udemy/python]::
オンライン学習サービスを利用するのも効果的な手段のひとつですが、筆者は試したことがないのでわかりません。
周りの評判を聞いてみても良いでしょう。YouTubeを探すのもありです。

==== 本書でわからないことがあったら

https://github.com/Mishima-syk/py4chemoinformatics/issues[py4chemoinformaticsのissues]::
py4chemoinformaticsのissuesに質問していただければお答えします。わかりにくい場合だったら修正しますので、よりよくなってみんなハッピー。

https://qiita.com/[Qiita]::
Qiitaで探せば大抵答えが見つかるはずです。

https://stackoverflow.com/[stackoverflow]::
それでも答えが見つからなかったらsofで探すか質問しましょう

http://mishima-syk.github.io/[Mishima.syk]::
本書を書いている人たちが集まるコミュニティです。特に話題をPythonに限定していませんが、Pythonを使ったネタが多めです。かなりガチですが、初心者対応も万全でハンズオンに定評があります。質問されれば大体答えられます。

=== Jupyter notebookで便利に使おう

link:https://jupyter.org/[Jupyter notebook]を利用すると、コードを書いて結果を確認するということがとても簡単にできるようになります。

Jupyter notebookはWebブラウザーベースのツールで、コードだけではなくリッチテキスト、数式、なども同時にノートブックに埋め込めます。また結果を非常に綺麗な図として可視化することも容易にできます。つまり、化学構造やグラフも描画できるため、ケモインフォマティクスのためのプラットフォームとして使いやすいです。さらに、プログラミングの生産性を上げるような、ブラウザ上でコードを書くとシンタックスハイライトや、インデント挿入を自動で行ってくれたりという便利な機能もついているので、特に初学者は積極的に使うべきでしょう。

==== 使い方

terminal(Windowsではanaconda prompt)から

[source, bash]
----
$ jupyter notebook
----

と打てばJupyter Notebookが立ち上がります。本書ではこれ以降特に断らない限りJupyter Notebook上でのコードを実行することとします。

=== Pythonで機械学習をするために

ケモインフォマティクスに限らず、インフォマティクスを学ぶにあたり、機械学習は外せません。本書でもある程度の機械学習の知識があることを前提に進めていきます。Pythonで機械学習をするにはlink:https://scikit-learn.org/stable/[Scikit-learn]というライブラリを利用するのが定番であり、本書でも特に説明せずに利用していきますが、初学者のために参考となる書籍などをすすめておきます。

link:https://www.amazon.co.jp/dp/4873117984/[Pythonではじめる機械学習 ―scikit-learnで学ぶ特徴量エンジニアリングと機械学習の基礎]::
Pythonで機械学習をやるための基礎を学べます。数学的な表現があまりないので読みやすいです。

link:https://github.com/Mishima-syk/sklearn-tutorial[sklearn-tutorial]::
y-samaによるsklearnのチュートリアルハンズオンのjupyter notebookです。

<<<
== 4章: ケモインフォマティクスのための公開データベース
:imagesdir: images

この章ではケモインフォマティクスでよく使うデータベースを紹介します。

=== ChEMBL

link:https://www.ebi.ac.uk/chembl/[ChEMBL]はEBIのChEMBLチームにより維持管理されている医薬品及び開発化合物の結合データ、薬物動態、薬理活性を収録したデータベースです。データは主にメディシナルケミストリ関連のジャーナルから手動で抽出されており、大体3,4ヶ月に一度データの更新があります。

メディシナルケミストリ関連のジャーナルからデータを収集しているため、QSARに関連する情報や背景知識を論文そのものに求めることが可能であり、創薬研究をする際には有用です。

NOTE: ChEMBLはもともとはlink:http://chembl.blogspot.com/2009/11/faq-where-can-i-download-starlite.html[StARlite]という商用データベースでした

=== PubChem

link:https://pubchem.ncbi.nlm.nih.gov/[PubChem]はNCBIにより維持管理されている低分子化合物とその生物学的活性データを収録している公開リポジトリです。5000万件以上の化合物情報と、100万件を超えるアッセイデータを含みそのデータ量の多さが特徴とも言えます。もうひとつの特徴はデータをアカデミアからの化合物登録やアッセイ結果の登録により成長することであり、ここが先のChEMBLとの大きな違いであるといえます。

特にPubChemは初期スクリーニングのデータが多いため、そのようなデータに対しなんらかのマイニングや分析を行いたい場合は有用だと考えられます。

どちらを使うべき?::
QSARをやりたい場合にはやはりChEMBLのデータを利用することが多いです。IC50のようなデータが得られていることが多いですし、モデルの解釈に元論文をあたることができるというのが大きな理由です。

=== ChEMBLで欲しい情報を検索する

NOTE: ChEMBLはユーザーインターフェースを刷新中で現在beta版のテストを行っていますが、いずれこちらに置き換わると思うので新バージョンのインターフェースでの検索方法を紹介します。

まずはlink:https://www.ebi.ac.uk/chembl/[ChEMBL]にアクセスし、画面上部のCheck out our New Interface (Beta). というリンクをクリックして新しいインターフェース画面に移行します。

image::ch04/chembl01.png[ChEMBL]

ChEMBLのデータは主に4つのカテゴリに分かれていて、一意なIDが振られており相互に関連付けされています。それぞれのカテゴリについて簡単に説明すると

Targets::
ターゲット分子についてその分子を対象としてアッセイされた論文に関してまとめられており、どういったジャーナルに投稿されているかや、どの年に投稿されたのかといった情報がまとめられています。また、アッセイに関しても同様にまとめられています。
Compounds::
化合物に関する基本的な物理量（分子量など）のほか、Rule of 5を満たしているかといった分子の特性情報や、臨床情報などの創薬関連情報のほか、ChEMBLでの関連アッセイ、関連論文のサマリがまとめられています。
Assays::
アッセイに関する情報と元論文との関連付けがされているほか、アッセイに供された化合物データへのリンクが貼られています。
Documents::
論文のタイトル、ジャーナル名、アブストラクトの他に関連論文データへのリンクと、その論文中で行われたアッセイへのリンクと使われた化合物データへのリンクが貼られています。

==== あるターゲットの関連化合物を探したい場合

ある創薬ターゲット分子がどのくらい研究開発されているかを知るために、それをターゲットとしてどのくらいの化合物が合成されたのか？さらに骨格のバリエーションはどのくらい存在するのかを調べたい場合がよくあります。ChEMBLを利用するとターゲット名で探索して関連化合物をダウンロードすることができます。

ここでは抗がん剤のターゲットとして知られているTopoisomerase2を検索します。画面上部のフォームにtopoisomeraseと入力して検索するとスクリーンショットのように表示されるはずです。

image::ch04/chembl02.png[ChEMBL]

サジェスト機能による絞り込みでいくつか候補をリスト表示してくるのでTOP2Bを選んでください。画面をスクロールするとAssociated Compoundsセクションがありますのでグラフのタイトル(Associated Compounds for Target CHEMBL3396)をクリックすると関連化合物一覧画面が開きます。

image::ch04/chembl03.png[ChEMBL]

259化合物存在することがわかります。スクロールすると全体をみることができます。画面右のアイコンをクリックするとそれぞれCSV(カンマ区切りテキスト),TSV(タブ区切りテキスト),SDF(5章で説明しています)の形式でダウンロードできます。

image::ch04/chembl04.png[ChEMBL]

==== あるアッセイの活性値と化合物が欲しい場合

QSARモデルを作る場合、アッセイの活性値と対応する化合物の構造情報が必要です。ChEMBLの場合アッセイのページからダウンロードすることでQSARモデル作成のためのデータを得ることができます。

大体次のような手順を辿ることがおおいです。

- 論文データを検索してからそれに関連付けられているアッセイデータを辿る
- ターゲットを検索してそれに紐付いているアッセイデータからQSARに使えそうなものを選ぶ

ここでは後者のターゲットから検索してQSARモデルに使えそうなアッセイデータを探します。心毒性関連ターゲットとしてよく知られているhERGのQSARモデルを作りたいという状況を想定しています。

検索フォームにhERGと入力して、Search hERG for all in Assaysを選びます。361件ヒットしました。

image::ch04/chembl05.png[ChEMBL]

モデル構築のためのデータが欲しいのでデータ数が多い順に並べ替えます。ヘッダーのCompoundsをクリックして降順に並べ替えます。

image::ch04/chembl06.png[ChEMBL]

論文由来で最もアッセイ数の多いCHEMBL829152を選んでクリックしてアッセイページを開きます。Activity chartの円グラフをクリックすると詳細画面が開くのでSelect allで全選択してTSV形式でダウンロードします。

image::ch04/chembl07.png[ChEMBL]

NOTE::
****
ダウンロードしたファイルをエディタで開くと\^@C^@h\^@E^@M\^@B^@L^@と文字化けすることがあります。これはutf-16-leでエンコードしているためです(こうしないとExcelで問題が発生するようです)。

viの場合':e ++enc=utf16le'と打てばきちんと表示されるようになります。
****

=== その他有用なデータベース

のちほど追記していきます。IssueやPRでも受け付けてます。

<<<
== 5章: RDKitで構造情報を取り扱う
:imagesdir: images

image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch05_rdkit.ipynb"]
image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch05_hetero_shuffle.ipynb"]

この章ではRDKitを使って分子の読み込みの基本を覚えます。

=== SMILESとは

Simplified molecular input line entry system(SMILES)とは化学構造を文字列で表現するための表記方法です。
詳しくはlink:http://www.daylight.com/meetings/summerschool98/course/dave/smiles-intro.html#TOC[SMILES Tutorial]で説明されていますが、例えばc1ccccc1は6つの芳香族炭素が最初と最後をつないでループになっている構造、つまりベンゼンを表現していることになります。

=== 構造を描画してみよう

SMILESで分子を表現することがわかったので、SMILESを読み込んで分子を描画させてみましょう。まずはRDKitのライブラリからChemクラスを読み込みます。二行目はJupyter Notebook上で構造を描画するための設定です。

[source, python]
----
from rdkit import Chem
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Draw
----

RDKitにはSMILES文字列を読み込むためにMolFromSmilesというメソッドが用意されていますので、これを使い分子を読み込みます。

[source, python]
----
mol = Chem.MolFromSmiles("c1ccccc1")
----

続いて構造を描画しますが、単純にmolを評価するだけで構造が表示されます。

[source, python]
----
mol
----

図のように構造が表示されているはずです。

image::ch05/ch05_01.png[Depict benzene]

NOTE: 余談ですが、jupyter notebookを使うとコードを書きながら構造を確認することが簡単にできるのでケモインフォマティクスにおいては重宝されます。

=== 複数の化合物を一度に取り扱うには？

複数の化合物を一つのファイルに格納する方法にはいくつかありますが、sdfというファイル形式を利用するのが一般的です。

.sdfフォーマットとは？
****
MDL社で開発された分子表現のためのフォーマットにMOL形式というものがあります。このMOL形式を拡張したものがSDF形式です。
具体的にはMOL形式で表現されたものを"$$$$"という行で区切ることにより、複数の分子を取り扱えるようにしてあります。
****

==== sdfファイルをChEMBLからダウンロードする

4章を参考にlink:https://www.ebi.ac.uk/chembl/beta/[ChEMBL]のトポイソメラーゼII阻害試験(CHEMBL669726)の構造データをsdfファイル形式でダウンロードします。

NOTE::
****
具体的な手順はリンクのページを開いて、検索フォームにCHEMBL669726を入力すると検索結果が表示されるので、Compoundsタブをクリックします。その後、全選択してSDFでダウンロードするとgzip圧縮されたsdfがダウンロードされるので、gunzipコマンドまたは適当な解凍ソフトで解凍してください。それをch05_compounds.sdfという名前で保存します。
****

==== RDKitでsdfを取り扱う

RDKitでsdfファイルを読み込むにはSDMolSupplierというメソッドを利用します。複数の化合物を取り扱うことになるのでmolではなくmolsという変数に格納していることに注意

[source, python]
----
mols = Chem.SDMolSupplier("ch05_compounds.sdf")
----

何件の分子が読み込まれたのか確認します。数を数えるにはlenを使います。

[source, python]
----
len(mols)
----

34件でした。

==== 分子の構造を描画する

forループを使って、ひとつずつ分子を描画してもいいですが、RDKitには複数の分子を一度に並べて描画するメソッドが用意されているので、今回はそちらのMolsToGridImageメソッドを使います。

[source, python]
----
Draw.MolsToGridImage(mols)
----

image::ch05/ch05_04.png[MolsToGridImage]

===== (おまけ)参考までにループを回すやりかたも載せておきます。

[source, python]
----
from IPython.core.display import display
for mol in mols:
    display(mol)
----

==== Q&A

[qanda]
変数名を分子の数でmolやmolsに変えるのはなぜですか？::
  どういう変数を使うかの決まりはありませんが、見てわかりやすい変数名をつけることで余計なミスを減らし、生産性が上がります
MolsToGridImageで一行に並べる分子の数を変更することはできますか？::
  できます。molsPerRowというオプションが用意されています。Draw.MolsToGridImage(mols, molsPerRow=10)のように使います

<<<

=== RDKitを使ってヘテロシャッフリングをしてみる

化合物の物性を変換するアプローチの一つとしてヘテロシャッフリングがあります。これは置換基の位置関係は維持したまま、芳香環を形成する炭素、窒素、硫黄、酸素などの原子種を入れ替えて新しい分子を生み出す手法です。
ヘテロシャッフリングをすることで活性を維持しつつ、脂溶性などのプロパティを変化させてADMETの課題を解決する、または活性そのものを改善するなどの効果を期待します。実際の現場では単にシャッフルしたものを何でも作ることはなく、Docking Studyなどをして好ましい相互作用を持つ分子のみを選んだりします。その前段階としていろんな分子を生成することは一つのアプローチとしてあっても良いでしょう。
古いですがBMSの研究者らはlink:https://pubs.acs.org/doi/10.1021/ci9004964[Morph]というアルゴリズムを論文発表しています。彼らは入力分子を元にヘテロ原子を入れ込んだ分子を種々発生させ、新しいケミカルシリーズの探索をしています。
RDKITで同じように再帰的にヘテロシャッフルした分子を生成するコードを書いてみました。
下記が実際のコード部分です。芳香環を形成する原子のインデックスを取得し、総当たりでC, N, S, Oに変えていきます。変えたものに関してSanitizeMolのチェックかけてパスしたもののみを返します。recursive_replaceという関数が再帰的に原子を入れ替えます。Thresholdの値がオプションで設定できます。これは入力分子内の芳香環を形成する原子に対する芳香環を形成する窒素原子の数の割合です。これを設定しているのは、芳香環嬢の原子が全て炭素以外の原子になってしまうのは違和感があるからです。

[source, python]
----
def replace_atom(mol):
    res = []
    aro_idxs = [atom.GetIdx() for atom in mol.GetAromaticAtoms() if atom.GetDegree() < 3]
    for atm_num in [6, 7, 8, 16]:
        for idx in aro_idxs:
            cp_mol = copy.deepcopy(mol)
            cp_mol.GetAtomWithIdx(idx).SetAtomicNum(atm_num)
            p = Chem.MolFromSmarts("nnn")
            if cp_mol.HasSubstructMatch(p) != True:
                try:
                    smi = Chem.MolToSmiles(cp_mol)
                    smi.replace("[sH]", "s")
                    cp_mol = Chem.MolFromSmiles(smi)
                    Chem.SanitizeMol(cp_mol)
                    res.append(cp_mol)
                except:
                    pass
    return res
 
def recursive_replace(mols, check=set([]), thres=0.4):
    before_n = len(check)
    print(before_n)
    for mol in mols:
        replaced_mols = replace_atom(mol)
        for mol_conv in replaced_mols:
            aro_n = Fragments.fr_Ar_N(mol)
            aro_a = len(mol.GetAromaticAtoms())
            ratio = float(aro_n) / float(aro_a)
            if ratio < thres:
                smi = Chem.MolToSmiles(mol_conv)
                check.add(smi)
    after_n = len(check)
    print(before_n, after_n)
    if before_n < after_n:
        mols = [Chem.MolFromSmiles(mol) for mol in check]
        recursive_replace(mols, check=check)
    return [Chem.MolFromSmiles(smi) for smi in check]
----

実際に使ってみます。

[source, python]
----
# Gefitinib
mol1 = Chem.MolFromSmiles('COC1=C(C=C2C(=C1)N=CN=C2NC3=CC(=C(C=C3)F)Cl)OCCCN4CCOCC4')
#  Oxaprozin
mol2 = Chem.MolFromSmiles('C1=CC=C(C=C1)C2=C(OC(=N2)CCC(=O)O)C3=CC=CC=C3')
Draw.MolsToGridImage([mol1, mol2])
----

元の分子

image::ch05/ch05_05.png[query]

[source, python]
----
res = recursive_replace([mol1])
Draw.MolsToGridImage(res, molsPerRow=5)
----

変換後１

image::ch05/ch05_06.png[res1]

[source, python]
----
res = recursive_replace([mol2])
Draw.MolsToGridImage(res, molsPerRow=5)
----

変換後２

image::ch05/ch05_07.png[res2]

どうでしょうか。二つの分子の例を示しました。一つ目は66の芳香環であり、それを形成できる原子は炭素と、窒素のみの場合です。二つ目は５員環で炭素、窒素、硫黄、酸素が原子の候補として入る場合の例です。いずれのケースでも上記のコードでヘテロ原子がシャッフルされたものが生成されていると思います。興味のある方はコードを書き換えていろいろ検証してみてください。

参考文献

- https://pubs.acs.org/doi/10.1021/jm3001289
- https://pubs.acs.org/doi/10.1021/acs.jcim.8b00563== 6章: 化合物の類似性を評価してみる
:imagesdir: images

image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch06_similarity.ipynb"]

=== 化合物が似ているとはどういうことか？

２つの化合物が似ているとはどういうことでしょうか？なんとなく形が似ている？という表現は科学的ではありません。
ケモインフォマティクスでは類似度(一般的に0-100の値を取ります)や非類似度（距離）といった定量的な尺度により似ているか
どうかを評価します。

ここでは主に２つの代表的な尺度を紹介します。

==== 記述子

分子の全体的な特徴を数値で表現するものを記述子と呼びます。分子量や極性表面性（PSA）、分配係数(logP)などがあり、現在までに多くの記述子が提案されています。
これらの記述子の類似性を評価することで２つの分子がどのくらい似ているかを表現することが可能です。また分子全体の特徴を一つの数字で表現するために局所的な特徴はできないです。

NOTE:いくつかの記述子に関しては市販ソフトでないと計算できない場合があります。

==== フィンガープリント

もう一つがフィンガープリントです。フィンガープリントとは分子の部分構造を0,1のバイナリーで表現したもので部分構造の有無とビットのon(1),off(0)を対応させたものになります。
フィンガープリントには固定長FPと可変長FPの二種類が存在し、古くはMACSKeyという固定長FP(予め部分構造とインデックスが決められているFP)が使われていましたが、現在ではECFP4(Morgan2)という可変長FPが利用されるのが普通です。

RDKitのフィンガープリントに関してはlink:https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf[開発者のGregさんのスライド]が詳しいので熟読してください。

今回はこのECFP4(Morgan2)を利用した類似性評価をしてみましょう。

=== 類似度を計算する

まずは手始めに簡単な分子としてトルエンとクロロベンゼンの類似性を評価してみましょう。

[source, python]
----
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, Draw
from rdkit.Chem.Draw import IPythonConsole
----

smilesで分子を読み込みます。

[source, python]
----
mol1 = Chem.MolFromSmiles("Cc1ccccc1")
mol2 = Chem.MolFromSmiles("Clc1ccccc1")
----

一応目視で確認しておきます。

[source, python]
----
Draw.MolsToGridImage([mol1, mol2])
----

ECFP4に相当する半径2のモルガンフィンガープリントを生成します。

[source, python]
----
fp1 = AllChem.GetMorganFingerprint(mol1, 2)
fp2 = AllChem.GetMorganFingerprint(mol2, 2)
----

類似度の評価にはタニモト係数を使います。

[source, python]
----
DataStructs.TanimotoSimilarity(fp1, fp2)
# 0.5384615384615384
----

=== （おまけ）もう少しドラッグライクな分子の類似性を評価する

ここでは抗凝固薬として上市されているlink:https://www.ebi.ac.uk/chembl/beta/compound_report_card/CHEMBL231779/[apixaban], link:https://www.ebi.ac.uk/chembl/beta/compound_report_card/CHEMBL198362/[rivaroxiaban]の類似性を評価します。
構造を見るとわかりますが、なんとなく似ていますが、どの部分とどの部分が対応するか
想像つくでしょうか？実はこの２つの化合物は両方共FXaというセリンプロテアーゼの同じポケットに
同じような結合モードで結合することでプロテアーゼの働きを阻害することが知られています。興味があれば
実際にPDBから複合体の結晶構造を探して眺めてみるといいかもしれません。
（pymol入門まで拡張するか？要検討）

[source, python]
----
apx = Chem.MolFromSmiles("COc1ccc(cc1)n2nc(C(=O)N)c3CCN(C(=O)c23)c4ccc(cc4)N5CCCCC5=O")
rvx = Chem.MolFromSmiles("Clc1ccc(s1)C(=O)NC[C@H]2CN(C(=O)O2)c3ccc(cc3)N4CCOCC4=O")
----

構造を眺めてみます。メトキシフェニルとクロロチオールは同じような結合様式をとるんでしょうか？このような結合の成分をきちんと評価する方法もあるのですが、本書の内容を超えるので説明はしません。もし興味があればFragment Molecular Orbital Methodで調べてみてください

[source, python]
----
Draw.MolsToGridImage([apx, rvx], legends=["apixaban", "rivaroxaban"])
----


[source, python]
----
apx_fp = AllChem.GetMorganFingerprint(apx, 2)
rvx_fp = AllChem.GetMorganFingerprint(rvx, 2)
----


[source, python]
----
DataStructs.TanimotoSimilarity(apx_fp, rvx_fp)
# 0.40625
----

40%くらいの類似度ということになりました。

<<<
== 7章: グラフ構造を利用した類似性の評価
:imagesdir: images

image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch07_MCS.ipynb"]

グラフとはノード（頂点）群とノード間の連結関係を示すエッジ（枝）群で構成されるデータのことを指します。化学構造はこのグラフで表現できます。つまり原子をノード、結合をエッジとしたグラフ構造で表せます。

通常、6章で紹介したようなフィンガープリントを使い分子同士の類似性を評価することが多いですが、グラフ構造を利用して類似性を評価する手法もあります。次に紹介するMCS（Maximum Common Substructure）は対象となる分子集合の共通部分構造のことを指します。共通部分構造が多いほとそれらの分子はより似ていると考えます。

=== 主要な骨格による分類(MCS)

最大共通部分構造Maximum Common Substructure(MCS)とは与えられた化学構造群において共通する最大の部分構造のことです。RDKitではMCS探索のためにrdFMCSというモジュールが用意されています。

今回はMCS探索のサンプルデータとしてrdkitに用意されているcdk2.sdfというファイルを利用します。RDConfig.RDDocsDirが、サンプルデータのディレクトリを表す変数で、そのディレクトリ以下のBooks/data/にcdk2.sdfというファイルが存在するので、os.path.joinメソッドでファイルパスを設定します。尚、os.path.joinはosのパスの違いを吸収するためのpythonの組み込みモジュールです。

[source, python]
----
import os
from rdkit import Chem
from rdkit.Chem import RDConfig
from rdkit.Chem import rdFMCS
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Draw
filepath = os.path.join(RDConfig.RDDocsDir, 'Book', 'data', 'cdk2.sdf')
mols = [mol for mol in Chem.SDMolSupplier(filepath)]
# 構造を確認します
Draw.MolsToGridImage(mols[:7], molsPerRow=5)
----

image::ch07/mcs01.png[compounds]

読み込んだ分子を使ってMCSを取得します。RDKitではMCSの取得方法に複数のオプションが指定できます。
以下にそれぞれのオプションでの例を示します。

. デフォルト
. 原子がなんであっても良い（構造とボンドの次数があっていれば良い）
. 結合次数がなんでも良い（例えば、ベンゼンとシクロヘキサンは同じMCSとなる）

[source, python]
----
result1 = rdFMCS.FindMCS(mols[:7])
mcs1 = Chem.MolFromSmarts　(result1.smartsString)
mcs1
print(result1.smartsString)
#[#6]1:[#7]:[#6](:[#7]:[#6]2:[#6]:1:[#7]:[#6]:[#7]:2)-[#7]
----

image::ch07/mcs02.png[MCS01]

[source, python]
----
result2 = rdFMCS.FindMCS(mols[:7], atomCompare=rdFMCS.AtomCompare.CompareAny)
mcs2 = Chem.MolFromSmarts(result2.smartsString)
mcs2
print(result2.smartsString)
#[#6]-,:[#6]-,:[#6]-[#6]-[#8,#7]-[#6]1:[#7]:[#6](:[#7]:[#6]2:[#6]:1:[#7]:[#6]:[#7]:2)-[#7]
----

image::ch07/mcs03.png[MCS02]

[source, python]
----
result3 = rdFMCS.FindMCS(mols[:7], bondCompare=rdFMCS.BondCompare.CompareAny)
mcs3 = Chem.MolFromSmarts(result3.smartsString)
mcs3
print(result3.smartsString)
#[#6]1:[#7]:[#6](:[#7]:[#6]2:[#6]:1:[#7]:[#6]:[#7]:2)-[#7]
----

image::ch07/mcs04.png[MCS03]

RDKitではMCSに基づく類似性を数値化するアルゴリズムのひとつにFraggle Similarityが実装されています。これを利用することでクラスタリングや、類似性に基づいた解析が行なえます。

[source, python]
----
from rdkit.Chem.Fraggle import FraggleSim
sim, match = FraggleSim.GetFraggleSimilarity(mols[0], mols[1])
print(sim, match)
#0.925764192139738 *C(C)C.*COc1nc(N)nc2[nH]cnc12
match_st = Chem.MolFromSmiles(match)
match_st
----

image::ch07/mcs05.png[FraggleSimilarity]

このようにFraggleSimilarityは類似性及びマッチした部分構造を返します。ECFPを利用した類似性よりもケミストの感覚に近いことが多いです。詳しくは参考リンクを参照してください。

参考リンク

- https://pubs.acs.org/doi/abs/10.1021/acs.jcim.5b00036[Efficient Heuristics for Maximum Common Substructure Search]
- https://raw.github.com/rdkit/UGM_2013/master/Presentations/Hussain.Fraggle.pdf[Fraggle – A new similarity searching algorithm]

=== Matched Molecular Pairによる化合物ネットワーク

創薬研究の構造最適化ステージにおいて、起点となる化合物（リード化合物）をどのように構造を変換していくかは非常に重要な問題です。加えてステージが進んだ場合どの構造変換が活性や物性に影響を及ぼしたかというレトロスペクティブな解析することも大切です。

TIP: 興味があればlink:https://sar.pharm.or.jp/wp-content/uploads/2018/09/SARNews_19.pdf[https://sar.pharm.or.jp/wp-content/uploads/2018/09/SARNews_19.pdf]を読むとよいです。

このような解析を行うためのアプローチの一つがMatched Molecular Pair Analyisis（MMPA)です。MMPAでは、二つの分子の活性、物性の変化と部分構造の変化を比較し解析します。例えばフェニル基上に置換基、Cl基->F基に変換した場合、活性、物性にどのような変化があるかを調べます。もし変換の前後で活性は変化せず物性パラメータが大きく変化したらそれは生物等価体と見なせます。MMPは基本的には変換した部位に着目する解析手法であり、大規模なデータを利用することでパラメータの変動のトレンドの把握ができます。

ここではRDKitのContribに提供されているlink:https://github.com/rdkit/rdkit/tree/master/Contrib/mmpa[RDKit/Contrib/MMPA][mmpa]を使ってMMP解析を行います。

作業ディレクトリをRDKitインストール先の下にあるContrib/mmpaに移し、pythonスクリプトを順次実行します。

[source, python]
----
python rfrag.py <MMPAを実施したいSmilesFileの名前 >フラグメント化したデータの保存ファイル名
# 例えば
# python rfrag.py <data/sample.smi >data/sample_fragmented.txt

python indexing.py <先のコマンドでできたフラグメントのファイル >MMP_アウトプットファイル.CSV
# 例えば
# python index.py <data/sample_fragmented.txt >data/mmp.csv
----

以上のコマンドを実行するとmmp.csvに分子A,分子B,分子AのID,分子BのID,変換された構造のSMIRKS,共通部分構造（context）が出力されます。ペアのIDが出力されていますのでこれに活性や物性などの評価値を紐つけることで構造変化と評価結果の変動の解析を実施することができます。
MMPは変換前、変換後の情報をノード、変換ルールをエッジと考えるとグラフ構造です。Cytoscapeなどのネットワーク可視化ツールを利用するとMMPAの結果をより直感的に把握することができます。

メディシナルケミストは、通常ある仮説に基づき構造変換の戦略を練ります。例えばある分子Mの部分構造Xに着目しその脂溶性を変化させるためにXをAに、その結果をもとにBに、さらにそれを元にCにと最適化を進めます。

これをMMP的に解釈すると、MX>>MA, MA>>MB, MB>>MCとなりますね。これを繋げるとMX>>MA>>MB>>MCと一続きになります。
このネットワークは構造変換の変遷を表現する手法の一つです。これを大規模なデータセットで実施すると、あるプロパティの変化と構造変換のルールを理解できます。MMPは基本的に部分構造変換にのみ着目するため、Scaffoldによらない普遍的なルールが見いだせるかもしれない！という期待があります。

その他、Matched Molecular SeriesというMMPの拡張的手法も提案されています。他には連続的な変換ルールを解析し、経験的に生み出されたlink:https://www.slideshare.net/NextMoveSoftware/revising-the-topliss-tree[Topless Tree]のルールを表現したlink:https://pubs.acs.org/doi/10.1021/jm500022q[論文]もあります。

ペアのデータを拡張し、トレンドを見極めるのがMMPAです。

=== Cytoscapeを使ってMMPネットワークを可視化する

WARNING: この内容は入門の内容を超えるので興味がなければ飛ばしてください

RDKitには先に紹介したMMPAの他にlink:https://github.com/rdkit/mmpdb[mmpdb]という別プロジェクトがあります。
こちらはコマンドラインのツール群とデータベースシステムとして提供されているため、長期的な管理がしやすいという特徴があります。本セクションではこのmmpdbとCytoscapeを利用したMMPの可視化を紹介します。

==== Cytoscapeのインストール

link:https://cytoscape.org/[Cytoscape]はオープンソースのネットワーク可視化ソフトで色々なシーンで広く使われています。化合物の構造表示用プラグインを使うことで構造のネットワークを表示することができます。

インストールは簡単でlink:https://cytoscape.org/download.html[ダウンロードサイト]から対応するOSのインストーラをダウンロードして指示のとおりにインストールするだけです。

インストールが完了したらCytoscapeを起動して化合物構造描画用のChemviz2プラグインをインストールします。手順は簡単でApps->App Managerからchemviz2を選択してインストールします。

image::ch07/chemviz2.png[AppManager, width=400]

==== mmpdbからgmlファイルを作成する

今回利用するデータはlink:https://www.ebi.ac.uk/chembl/assay/inspect/CHEMBL930273[<Inhibition of recombinant GSK3-beta> J. Med. Chem. (2008) 51:2062-2077]の151化合物です。MMPAを行うにはHTSのような探索データではなくて構造最適化のようにスキャフォールドが決まっているものを使うのが原則です。

コマンドの流れを載せておきます。smilesのtextと活性や物性値のデータは別々にデータベースに登録する必要があります。

[source, bash]
----
$ mmpdb fragment smiles.txt -o CHEMBL930273.fragments     # fragmentation
$ mmpdb index CHEMBL930273.fragments -o CHEMBL930273.db   # make db
$ mmpdb loadprops -p act.txt CHEMBL930273.db              # load properties
----

そのあとCytoscapeで読み込むためのgmlファイルを作成しますが、これは本書の範囲を超えるので割愛します。もし興味があるのであればlink:https://github.com/Mishima-syk/12/tree/master/kzfm[コード]を直接読んでもらうといいのですが
流れは以下のとおりです。

. link:https://github.com/Mishima-syk/12/blob/master/kzfm/mmp2gml.py[mmpdbからpython-igraphをつかてgmlファイルを作る]
. link:https://github.com/Mishima-syk/12/blob/master/kzfm/CHEMBL930273.gml[gmlファイル]をCytoscapeで読み込む
. Cytoscapeで属性をいい感じにいじる
.. ノードの大きさを物性値に対応
.. エッジの色を活性差に対応
.. chemviz2 pluginで構造を描画してノードに貼り付ける

==== 解釈する

さてMMPネットワークを見てみましょう。あまり活性差のないMMPが左上の方に固まっています。右下の方にはエッジが赤い（活性差が大きい）ものが観測されます。このような小さな置換基変化が大きな活性差を生むもMMPをActivity Cliffと呼びます。一般的にActivity Cliffは創薬プロジェクトにおいてブレークスルーとなることが多いのでこういう活性変化を見逃さないことが大切です。

image::ch07/mmp01.png[MMPN, width=600]

実際にどういう置換が行われたのかを確認すると、OH基をMeO基に置換することで活性が消失しています。

image::ch07/mmp02.png[MMPN, width=600]

MMPだけではこのように単純に事実しかわかりませんが、今回はもう少し深く考察するために類似体の複合体結晶構造を探してみました。するとlink:https://www.rcsb.org/structure/5OY4[PDBID:5OY4]というGSK3βと類似化合物の複合体が見つかりました。

image::ch07/mmp03.png[MMPN, width=600]

OH基をMeO基に置換するとポケットの壁にぶつかりそうですね。したがってこのActivity Cliffはリガンドと蛋白質の立体障害
により引き起こされたと考察されます。

image::ch07/mmp04.png[MMPN, width=600]

MMPを視覚化して解釈する例を紹介しました。

==== 参考資料

- link:https://github.com/Mishima-syk/12/tree/master/kzfm[MMP visualization with Cytoscape]
- link:https://www.slideshare.net/KazufumiOhkawa/mishimasyk141025[Cytoscapeでchemoinformatics]
- link:https://chemrxiv.org/articles/mmpdb_An_Open_Source_Matched_Molecular_Pair_Platform_for_Large_Multi-Property_Datasets/5999375[mmpdb: An Open Source Matched Molecular Pair Platform for Large Multi-Property Datasets]

<<<
== 8章: 沢山の化合物を一度にみたい
:imagesdir: images

image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch08_visualization.ipynb"]

沢山のデータがどのように分布しているのかを見るには適当な空間にマッピングするのが一般的です。特にケモインフォマティクスではケミカルスペースという言葉が使われます。

=== Chemical Spaceとは

ケミカルスペースとは化合物を何らかの尺度でn次元の空間に配置したものを指します。一般に、2次元または3次元が使われることが多いです（人間の理解のため）。尺度つまり類似性に関しては色々な手法が提案されていますが、うまく化合物の特徴を表すような距離が定義されるように決められることが多いです。

ここではいくつかの尺度を用いてケミカルスペースを構築してみましょう。使うデータはlink:https://www.ebi.ac.uk/chembl/assay/inspect/CHEMBL3705525[ChEMBLのFXa酵素阻害アッセイに含まれる122化合物]です。

アッセイから全化合物表示をします。

image::ch08/ch08_1.png[Screenshot1, size=500]

続いてタブ区切りテキストでダウンロードします。

image::ch08/ch08_2.png[Screenshot2, size=500]

この中にSMILES形式で構造情報が入っています。

=== ユークリッド距離を用いたマッピング

描画ライブラリにはggplotを使います。まだインストールされていない場合はconda install ggplotでインストールしてください。主成分分析(PCA)を利用して、化合物が似ているものは近くになるように分布させて可視化します。

[source, python]
----
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, Draw
import numpy as np
import pandas as pd
from ggplot import *
from sklearn.decomposition import PCA
----

タブ区切りテキストを読み込み、SMILESの領域を取り出してMolFromSmilesメソッドでmolオブジェクトに変換します。

[source, python]
----
mols = []
with open("ch08_compounds.txt") as f:
    header = f.readline()
    smiles_index = -1
    for i, title in enumerate(header.split("\t")):
        if title == "CANONICAL_SMILES":
            smiles_index = i
    for l in f:
        smi = l.split("\t")[smiles_index]
        mol = Chem.MolFromSmiles(smi)
        mols.append(mol)
----

molオブジェクトからフィンガープリントを構築しますが、sklearnで取り扱えるようにnumpyアレイに変換します。
rdkitのフィンガープリントをsklearnで取り扱う場合にはこの変換操作が必須なのでそういうものだとして覚えてしまうと
よいです。

[source, python]
----
fps = []
for mol in mols:
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)
    arr = np.zeros((1,))
    DataStructs.ConvertToNumpyArray(fp, arr)
    fps.append(arr)
fps = np.array(fps)
----

これで主成分分析の用意が整ったので早速やりましょう。主成分の数はn_componentsで指定できます。

[source, python]
----
pca = PCA(n_components=2)
x = pca.fit_transform(fps)
----

描画します。

[source, python]
----
d = pd.DataFrame(x)
d.columns = ["PCA1", "PCA2"]
g = ggplot(aes(x="PCA1", y="PCA2"), data=d) + geom_point() + xlab("PCA1") + ylab("PCA2")
----

image::ch08/pca.png[PCA, size=500]

=== tSNEをつかったマッピング

PCAよりもtSNEのほうが分離能がよく、メディシナルケミストの感覚により近いと言われています。

[source, python]
----
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=0)
tx = tsne.fit_transform(fps)
----

描画します。

[source, python]
----
d = pd.DataFrame(tx)
d.columns = ["PCA1", "PCA2"]
g = ggplot(aes(x="PCA1", y="PCA2"), data=d) + geom_point() + xlab("PCA1") + ylab("PCA2")
----

image::ch08/tsne.png[PCA, size=500]

今回紹介したPCA,tSNEの他にも色々な描画方法があるので調べてみるとよいでしょう。

<<<
== 9章: 構造活性相関（QSAR）の基礎

image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch09_qsar.ipynb"]

化学構造と生物学的活性における相関関係をStructure Activity Relationship(SAR)またはQuantative SAR(QSAR)と呼びます。一般的には**似たような化合物は似たような生物学的活性を示す**ことが知られており、この相関関係を理解しドラッグデザインに活かすことが創薬研究において大変重要です。

また、このような問題には細胞の生死、毒性の有無といった化合物がどのクラスに入るのかを推定する分類問題と阻害率（%inhibition）といった連続値を推定する回帰問題の2つがあります。

=== 効果ありなしの原因を考えてみる（分類問題）

ChEMBからlink:https://www.ebi.ac.uk/chembl/assay/inspect/CHEMBL829152[hERG阻害アッセイ]の73データを用いてIC50が1uM未満のものをhERG阻害あり、それ以外をhERG阻害なしとラベルします。

まずは必要なライブラリをインポートします。

[source, python]
----
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, Draw
from rdkit.Chem.Draw import IPythonConsole
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.ensemble import RandomForestClassifier
----

ChEMBLでダウンロードしたタブ区切りテキストの処理は8章とほぼ同じですが、今回は活性データが欲しいのでSTANDARD_VALUEという列を探して数値を取り出します。この値が1000nM未満であればPOSというラベルを、そうでなければNEGというラベルを振ります。最後にラベルをnumpy arrayにしておきます。

[source, python]
----
mols = []
labels = []
with open("ch09_compounds.txt") as f:
    header = f.readline()
    smiles_index = -1
    for i, title in enumerate(header.split("\t")):
        if title == "CANONICAL_SMILES":
            smiles_index = i
        elif title == "STANDARD_VALUE":
            value_index = i
    for l in f:
        ls = l.split("\t")
        mol = Chem.MolFromSmiles(ls[smiles_index])
        mols.append(mol)
        val = float(ls[value_index])
        if val < 1000:
            labels.append("POS")
        else:
            labels.append("NEG")

labels = np.array(labels)
----

続いてmolオブジェクトをフィンガープリントに変換します。このフィンガープリントからhERG阻害の有無を予測するモデルを作成します。

[source, python]
----
fps = []
for mol in mols:
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)
    arr = np.zeros((1,))
    DataStructs.ConvertToNumpyArray(fp, arr)
    fps.append(arr)
fps = np.array(fps)
----

データセットを訓練セットテストセットの2つに分けます。テストセットは作成した予測モデルの精度を評価するためにあとで使います。

[source, python]
----
x_train, x_test, y_train, y_test = train_test_split(fps, labels)
----

予測モデルを作成するにはインスタンスを作成してfitメソッドで訓練させるだけです

[source, python]
----
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
----

先程分割しておいたテストセットを予測します。

[source, python]
----
y_pred = rf.predict(x_test)
----

confusion matrixを作成します。

[source, python]
----
confusion_matrix(y_test, y_pred)
----

f1スコアを見てみましょう。

[source, python]
----
f1_score(y_test, y_pred, pos_label="POS")
----

あまりよくないですね。

=== 薬の効き目を予測しよう（回帰問題）

回帰モデルは最初に説明したとおり、連続値を予測するモデルとなります。今回はRandomForestの回帰モデルを作成して、その精度をR2で評価します。データは分類問題で使ったhERGのアッセイデータを利用することにしましょう。最初に必要なライブラリをインポートします。

[source, python]
----
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from math import log10
----

分類問題のときにはラベル化しましたが、今度は連続値を予測したいのでpIC50に変換します。(なぜpIC50にすると都合が良いのかはそのうち補足する)

[source, python]
----
pIC50s = []
with open("ch09_compounds.txt") as f:
    header = f.readline()
    for i, title in enumerate(header.split("\t")):
        if title == "STANDARD_VALUE":
            value_index = i
    for l in f:
        ls = l.split("\t")
        val = float(ls[value_index])
        pIC50 = 9 - log10(val)
        pIC50s.append(pIC50)

pIC50s = np.array(pIC50s)
----

データセットをトレーニングセットとテストセットの2つに分割します。フィンガープリントは分類モデルのときに作成したものを流用します。

[source, python]
----
x_train, x_test, y_train, y_test = train_test_split(fps, pIC50s)
----

訓練します。Scikit-learnの場合はこの手順はどの手法でもほぼ同じメソッドでfitしてpredictです。

[source, python]
----
rf = RandomForestRegressor()
rf.fit(x_train, y_train)
----

予測しましょう。

[source, python]
----
y_pred = rf.predict(x_test)
----

予測精度をR2で出してみます。

[source, python]
----
r2_score(y_test, y_pred)
----

まずまずといったところでしょうか。

=== モデルの適用範囲(applicability domain)

今回紹介した手法は**似たような化合物は似たような生物学的活性を示す**という仮設に基づいて生成されるモデルです。もしトレーニングセットに似ている化合物が含まれなかった場合の予測精度はどうなるのでしょうか？

当然その場合は予測された値は信頼できませんよね。つまり、予測値にはその予測が確からしいか？という信頼度が常についてまわります。そのようなモデルが信頼できる、または適用できる範囲をapplicability domainと呼びます。これに関しては明治大学金子先生のlink:https://datachemeng.com/applicabilitydomain/[モデルの適用範囲・モデルの適用領域]が詳しいです。

==== (おまけコラム)applicability domainは信頼できるのか？

applicability domainはトレーニングセットの類似性からその予測が信頼できるかという確度を測る手法です。ここで類似性が誰のための類似性なのかという問題が出てきます。我々がこの化合物とこの化合物は似ているよねと思うのは我々の勝手ですが、似ているか似ていないかは最終的には蛋白質が判断します。このあたりはいわゆるインピーダンスミスマッチだと考えています。MMPの文脈で説明されるActivity Cliffなどインピーダンスミスマッチに気取った名前をつけただけではないでしょうか？

筆者はHugo Kubinyi先生の**似ている化合物は果たして似た活性を示すのか？**という疑問を、estradiolのOH基がMetxy基に変換すると活性が消失する例を上げて説明されているコメントに感銘を受けたのを覚えています。J.M.Cだと思うんですが、探しても見つからないのでもしご存知でしたら教えてください。

<<<
== 10章: ディープラーニング入門
:imagesdir: images

link:ch09_qsar.asciidoc[9章]でRandomForestを用いたQSAR解析をしましたが、QSARに用いられるアルゴリズムはこれ以外にSupportVectorMachine(SVM), LogisticRegiression, ArtificialNeuralNetwork(ANN)などといったものが利用されています。

本章では、近年注目を浴びているANNの一種であるディープラーニグの概要に関して説明し、ディープラーニングフレームワークであるTensorflow/Kerasを離床してみます。

=== ディープラーニングに関して

生物の脳には神経細胞が存在し、それらがネットワークを形成することで情報を伝達したり、記憶や学習しています。このネットワーク構造を数理モデル化したものがArtificial Neural Network(ANN)です。

ANNは、学習のための情報を入れる**入力層**、入力情報のパターンを元に反応するかしないか（神経シナプスの発火に対応）を学習する**中間層（または隠れ層）**、最後の**出力層**の三層から構成されていますが、ディープラーニングはこの隠れ層の部分が分岐などを含む多層構造にすることで高精度な予測を可能としています。

ディープラニングは近年盛んに研究されており、上記のような多層構造を持つという単純なモデルだけではなく様々な構造のモデルが日々提案されています。それらすべてのフォローは本書の範囲を超えるのでこれ以上は説明しません。


=== TensorFlowとKerasについて

Pythonでディープラーニングを行う場合、複数のフレームワークがあります。主なものを挙げると

- link:http://deeplearning.net/software/theano/[Theano]
- link:https://www.tensorflow.org/[Tensorflow]
- link:https://keras.io/[Keras]
- link:https://mxnet.apache.org/[MXNet]
- link:https://chainer.org/[Chainer]
- link:https://pytorch.org/[PyTorch]

この中で本書ではGoogle社が開発しているフレームワークのTensorflow/Kerasを使います。

Tensorflowは最近1.xから2.xにメジャーアップデートをしていますが、2.x版はまだ登場したばかりで参考情報も少ないので、1.x系でまずは試しましょう。また同じ1.xでもバージョンによってAPIが少し変更されていますので、今後動かしたいコードがあった時にどのバージョンで書かれているかに気をつける必要があります。

KerasはTensoflowなどの低レベルフレームワークをバックエンドに動く高レベルAPIで、Kerasを利用することでテンポよくコードが書けます。KerasはもともとTensorflowとは独立して開発されてきましたが、最近のTensorflowはKerasを取り込んでいます。したがってTensorflow側からKerasを呼び出し、ネットワークを構築することができます。Kerasそのものを使うのが良いか、Tensorflow側のKerasを使うのが良いのか、悩ましいところです。Kerasで作ってきたコードをTensorflowに統合されたKerasに移行するのはそれほど大変ではありません。また、今後Tensorflow側のモジュールを使いたくなることもあるかもしれないので本書ではTensorflowに統合されたKerasを利用します。

=== インストールしてみよう

Tensorflow とKerasをインストールしてみましょう。
anacondaでインストールする場合、GPU対応バージョンを使うか、CPUバージョンを使うかでインストールするパッケージが少し異なります。

[source, bash]
----
# CPU版
$ conda install -c conda-forge tensorflow
# GPU版
$ conda install -c anaconda tensorflow-gpu
----

[source, bash]
----
$ conda install -c conda-forge keras 
----

この例ではcondaコマンドを利用しインストールしていますが、pipコマンドを利用しインストールすることもできます。その場合はlink:https://www.tensorflow.org/install[公式ドキュメント]を参照してください。しかし基本的にはCondaで環境を作ったらCondaでパッケージを入れることが望ましいでしょう。


参考リンク

- https://keras.io/#installation
- https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html

=== Google colabとは

.Google colab
****
link:https://colab.research.google.com/notebooks/welcome.ipynb[Google colaboratory]はクラウド上で実行できるJupyter notebook環境です。Theano, Thensorflow, Keras, Pytorchなどのディープラーニング用のフレームワークがインストール済みなのと時間の制限はありますがGPUが使えるため、手元にGPUマシンがなくてもディープラーニングを利用できる点が非常に魅力的です。

利用にはGoogleのアカウントを作成する必要があるので、もしGoogleアカウントを持っていなければこの機会にアカウントを取得し利用してみると良いでしょう。
****

Google のアカウントがある方はGithub状のノートブックをそのままColab上で実行することもできます。（*デフォルトで提供されていないライブラリもあるのでその場合は別途環境を作る必要があります。）ここでは、以前Mishima.sykで実施たScikit-learnハンズオンのノートブックを開いてみます。
まず、link:https://colab.research.google.com/notebooks/welcome.ipynb[Google colaboratory]にアクセスします。

image::ch10/ch10_1.png[GoogleColabTop]

次いで、Githubというタブをクリックし、以下のURLをコピペします。
link:https://github.com/Mishima-syk/sklearn-tutorial[https://github.com/Mishima-syk/sklearn-tutorial]
するとURL先のNotebookが読み込ます。本資料は@y_samaが作成してくれたものです。データの準備からはじまり、link:https://automl.github.io/auto-sklearn/master/[AutoSklearn] まで網羅されています。非常にクオリティが高くDeepLearningではないですが、機械学習を学びたい方にとっては良いマテリアルになると思います。
後半のPandasMLやAutoSklearnは、デフォルトの設定ですとCoalbに入ってないとエラーが出るかもしれません

image::ch10/ch10_2.png[GoogleColab2]

ノートブックを開くとJupyter notebookと同じ感じの画面になります。Shift＋リターンキーでセルのコードを実行できると思います。

image::ch10/ch10_2.png[NoteBook]

参考までに本書執筆時点（201903）でGoogleColabデフォルトで利用できるライブラリを確認するにはセルの中で '!pip freeze'と打つと列記されます。

- absl-py==0.7.0
- alabaster==0.7.12
- たくさん出てくる
- yellowbrick==0.9.1
- zict==0.1.3
- zmq==0.0.0

<<<
== 11章: ディープラーニングを利用した構造活性相関
:imagesdir: images

image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch11_simple_dnn.ipynb"]

link:https://github.com/Mishima-syk/py4chemoinformatics/blob/master/ch09_qsar.asciidoc[９章]で構造活性相関の基礎を学びました。link:https://github.com/Mishima-syk/py4chemoinformatics/blob/master/ch10_deeplearning.asciidoc[10章]でディプラーニングを学びました。本章では、早速DNNを利用して構造活性相関解析をします。

=== DNNを利用した予測モデル構築

はじめにDNNを利用したシンプルな予測モデルを構築してみます。ここでは9章と同じデータを使います。最初に分類モデルを作成し、Positiveのラベルを[0, 1], Negativeのラベルを[1, 0]の二次元のOneHotベクトルで表します。KerasのModelオブジェクトを利用してモデルを作成した場合、上記の二次元のそれぞれの期待値が得られます。どちらのクラスに属する可能性が高いかを知るにはNumpyのArgmax関数を使えばよいです。

TIP: このアプローチは次元が増えても同じで、１０クラス分類であれば10次元で各クラスの期待値が返ってくるので同様にArgmaxを使うことで最も期待値が大きいクラスのインデックスを取得できます。


必要なライブラリをインポートします。

[source, python]
----
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, Draw
from rdkit.Chem.Draw import IPythonConsole
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score
# DNN用のライブラリを読み込みます。 tensorflowに統合されたKerasを使うように変えました(20190306)
# from keras.layers import Input
# from keras.layers import Dense
# from keras.layers import Dropout
# from keras.layers import Activation
# from keras.models import Model
from thensorflow.python.keras.layers import Iput
from thensorflow.python.keras.layers import Dense
from thensorflow.python.keras.layers import Dropout
from thensorflow.python.keras.layers import Activation
from thensorflow.python.keras.Model import Model


----

次にデータを読み込みます。9章では”POS”/”NEG”をlabelsというリストに入れたので一次元表現でしたが、今回はここが二次元になっています。

[source, python]
----
mols = []
labels = []
with open("ch09_compounds.txt") as f:
    header = f.readline()
    smiles_index = -1
    for i, title in enumerate(header.split("\t")):
        if title == "CANONICAL_SMILES":
            smiles_index = i
        elif title == "STANDARD_VALUE":
            value_index = i
    for l in f:
        ls = l.split("\t")
        mol = Chem.MolFromSmiles(ls[smiles_index])
        mols.append(mol)
        val = float(ls[value_index])
        if val < 1000:
            labels.append([0,1]) # Positive
        else:
            labels.append([1,0]) # Negative
labels = np.array(labels)
----

続いて分類モデルと回帰モデルを順次作成します。

まずは回帰モデルで、入力は9章と同じECFPを利用しています。DNNの構築には入力データの次元を明示的に指定する必要があるためnBitsという変数を定義しています。 

TIP: train_test_splitにrandom_stateで適当な整数を指定すると毎回同じデータが得られるので検証の際に有用です。

[source, python]
----
nBits = 2048
fps = []
for mol in mols:
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=nBits)
    arr = np.zeros((1,))
    DataStructs.ConvertToNumpyArray(fp, arr)
    fps.append(arr)
fps = np.array(fps)

x_train1, x_test1, y_train1, y_test1 = train_test_split(fps, labels, random_state=794)
----

入力が2048次元、300ニューロンの全結合層が三層、最後の出力層が2となるニューラルネットワークを作成します。活性化関数にはReLU, 出力層には二次元の多クラス分類のためにSoftmaxを用いました。

Dropout層はランダムにニューロンを欠損させることにより過学習を防ぐ役割を果たします。

Kerasではモデルを定義した後compile関数を呼ぶことでモデルを構築します。optimizer, lossは目的に応じて変更する必要がありますが、今回は'categorical_crossentropy'を使いました。

TIP: link:https://en.wikipedia.org/wiki/Rectifier_(neural_networks)[ReLU]はlink:https://en.wikipedia.org/wiki/Sigmoid_function[Sigmoid]関数の勾配消失の課題を克服できるためよく利用されます。

TIP: link:https://keras.io/ja/optimizers/[optimzer]はadam以外にも多くあるのでどれが適切かは実際は試行錯誤が必要となるでしょう。

[source, python]
----
# Define DNN classifier model
epochs = 10
inputlayer1 = Input(shape=(nBits, ))
x1 = Dense(300, activation='relu')(inputlayer1)
x1 = Dropout(0.2)(x1)
x1 = Dense(300, activation='relu')(x1)
x1 = Dropout(0.2)(x1)
x1 = Dense(300, activation='relu')(x1)
output1 = Dense(2, activation='softmax')(x1)
model1 = Model(inputs=[inputlayer1], outputs=[output1])

model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
----

NOTE: Kerasにはlink:https://keras.io/ja/models/sequential/[Sequential]モデルが用意おり、これを使うことで上記の例（Functional API）よりもシンプルにネットワークを記述できます。今回Functional APIでモデルを定義したのは、こちらに慣れておくと入力が複数の場合やより複雑なモデルの構築にも対応しやすいからです。もしSequentialの書き方に興味がある方は公式サイトやQiitaを調べてください。

NOTE: DNNは初期のランダムに発生させた重みに基づいて予測した予測値と実際の値を比較し、その差（LOSS)を最小化するように重みを更新するBackprobagationという手順を繰り返しながらモデルを最適化します。この繰り返しの回数を指定するのがEpochsです。Epochsを増やすとどんどん賢くなるように思われるかもしれませんが、計算コストがかかることと、過学習のリスクもあるので長ければ良いというものでもありません。Loss/Accuracyなどを観測しつつ適切なEpoch数を考えましょう。

モデルを構築したら後はScikit-learnと同じ感覚でfit/predictが行えます。

[source, python]
----
hist1 = model1.fit(x_train1, y_train1, epochs=epochs)
----

最後に結果を可視化してみます。

[source, python]
----
%matplotlib inline
import matplotlib.pyplot as plt
plt.plot(range(epochs), hist1.history['acc'], label='acc')
plt.legend()
plt.plot(range(epochs), hist1.history['loss'], label='loss')
plt.legend()
----

今回の例ではだいたい6Epochくらいでモデルが良い精度になりました。

次にテストデータで検証します。

[source, python]
----
y_pred1 = model1.predict(x_test1)
y_pred_cls1 = np.argmax(y_pred1, axis=1)
y_test_cls1 =np.argmax(y_test1, axis=1)
confusion_matrix(y_test_cls1, y_pred_cls1)
----

ちょっと微妙でしょうか、、、

回帰モデルも基本的には先ほどの分類問題と同じです。今度は回帰なので最後の出力層は値そのもの、つまり一次元になります。また活性化関数はSigmoidなどでは0-1になってしまうのでLinearとしています。学習データは9章のコードを流用しています。

[source, python]
----
from math import log10
from sklearn.metrics import r2_score
pIC50s = []
with open("ch09_compounds.txt") as f:
    header = f.readline()
    for i, title in enumerate(header.split("\t")):
        if title == "STANDARD_VALUE":
            value_index = i
    for l in f:
        ls = l.split("\t")
        val = float(ls[value_index])
        pIC50 = 9 - log10(val)
        pIC50s.append(pIC50)

pIC50s = np.array(pIC50s)
x_train2, x_test2, y_train2, y_test2 = train_test_split(fps, pIC50s, random_state=794)
----

次にモデルを定義します。Lossの部分が先ほどの分類モデルとは異なり、MSEになっていることに注意して下さい。

[source, python]
----
epochs = 50
inputlayer2 = Input(shape=(nBits, ))
x2 = Dense(300, activation='relu')(inputlayer2)
x2 = Dropout(0.2)(x2)
x2 = Dense(300, activation='relu')(x2)
x2 = Dropout(0.2)(x2)
x2 = Dense(300, activation='relu')(x2)
output2 = Dense(1, activation='linear')(x2)
model2 = Model(inputs=[inputlayer2], outputs=[output2])
model2.compile(optimizer='adam', loss='mean_squared_error')
----

ここまでできたら後は同じです。

[source, python]
----
hist = model2.fit(x_train2, y_train2, epochs=epochs)
y_pred2 = model2.predict(x_test2)
r2_score(y_test2, y_pred2)
plt.scatter(y_test2, y_pred2)
plt.xlabel('exp')
plt.ylabel('pred')
plt.plot(np.arange(np.min(y_test2)-0.5, np.max(y_test2)+0.5), np.arange(np.min(y_test2)-0.5, np.max(y_test2)+0.5))
----

いかがでしょうか。予測モデルはちょっとUnderEstimate気味ですかね。DNNは重ねるレイヤーの数、ドロップアウトの割合、隠れ層のニューロンの数、活性化関数の種類など数多くのパラメータをチューニングする必要があります。今回の例は決め打ちでしたが、色々パラメータを変えてモデルの性能を比較してみるのも面白いです。

=== 記述子を工夫してみる(neural fingerprint)

さて、ここまで分子のフィンガープリントを入力としてRandomForestやDNNのモデルを作成してきました。DNNが大きく注目を浴びた理由の一つに人が特徴量を抽出しなくてもモデルが特徴量を認識してくれるということが挙げられます。

例えば画像の分類においては、からlink:https://en.wikipedia.org/wiki/Scale-invariant_feature_transform[SIFT]という特徴量を人が定義し、これを入力としたモデルが作られていましたが、現在のDNNにおいては基本的に画像のピクセル情報そのものを利用しています。

ケモインフォマティクスに置き換えてみると、SIFTは分子のフィンガープリントに相当します。ですのでここ(入力)をもっとPrimitiveな表現に変えることでDNNの性能が上がるのではないか？と考えるのは至極当然の流れです。2015年、Harvard大学の, Alan Aspuru-Guzikらのグループは一つのチャレンジとしてlink:https://arxiv.org/pdf/1509.09292.pdf[Neural Finger print/NFP]というものを提唱しました。

今まで利用してきたECFPとNFPとの違いを、彼らの論文中の図を引用して示します。

image::ch11/ch11_nfp.png[Neural Finger Print]

ECFP(Circular Fingerprints)は入力の分子それぞれの原子からN近傍（Nは任意）までの原子までの情報をHash関数（この例ではMod）任意の値に変換、で固定長のベクトルに直すといったものでした。ざっくりいうと部分構造の有無を0/1のビット情報に直したものを利用するといったイメージです。一方、今回紹介するNFPはECFPにコンセプトは似ているのですが、Hash関数の部分がSigmoidに、Modで離散化する部分がSoftmaxになっています。従って入力されるデータセットによりECFPよりも柔軟に分子のフィンガープリントを生成することが期待されます。

この論文が発表されて以降、数多くの実装がGithubに公開されていますが、各実装ごとにKerasでもBackendがTheanoであったり、Keras/Tensorflowであっても、Keras1.xじゃないと動作しなかったりと意外と環境依存のものが多く扱いにくい状況になっています。残念なことに今回構築した環境で動作するものが公開されていませんのでKeras2.x/Python3.6で動作するものをこちらのlink:https://github.com/keiserlab/keras-neural-graph-fingerprint[コード]をベースに作成しました。

[source, python]
----
git clone https://github.com/iwatobipen/keras-neural-graph-fingerprint.git
----

example.pyというファイルのコードを眺めるとなんとなく雰囲気がつかめると思います。分子の表現は、これまでの例はフィンガープリントをRDKitを使い生成していましたが、今回はこのフィンガープリントそのものをDNNが学習します。

ということで、分子をグラフとして表現したものが入力になります。Atom_matrixとして(max_atoms, num_atom_features)をEdge_matrixとして(max_atoms, max_degree)をbond_tensorとして(max_atoms, max_degree, num_bond_features)という三つの行列を使います。分子はそれぞれ原子数が異なるためmax_atomsで最大原子数を定義しています。こうすることで分子ごとに同一の行列サイズの入力となりバッチ学習が可能となります。

Exampleを実行するのであれば下記のコマンドを入力してください。

[source, python]
----
python example.py
----

参考リンク

- link:https://arxiv.org/abs/1509.09292[NGF-paper]
- link:https://arxiv.org/abs/1611.03199[DeepChem-paper]
- link:http://www.keiserlab.org/[keiserlab]
- link:https://github.com/HIPS/neural-fingerprint[HIPS NFP]
- link:https://github.com/debbiemarkslab/neural-fingerprint-theano[Theano base]
- link:https://github.com/GUR9000/KerasNeuralFingerprint[for keras1.x]
- link:https://github.com/ericmjl/graph-fingerprint[ericmjl/graph_fp]
- link:https://github.com/deepchem/deepchem[DeepChem]

<<<
== 12章: コンピューターに化学構造を考えさせる
:imagesdir: images
image:jupyter.png[link="https://github.com/Mishima-syk/py4chemoinformatics/blob/master/notebooks/ch12_rnn.ipynb"]

Deep Learningがメディナルケミストリに大きなインパクトをもたらしたものの一つに生成モデルがあげられます。特にこの数年での生成モデルの進化は素晴らしいです。ここではlink:https://github.com/MarcusOlivecrona/REINVENT[Marcus Olivecronaにより開発されたREINVENT]を使って新規な合成案を提案させてみましょう。

=== 準備
pytorchというディープラーニングのライブラリをcondaでインストールします。新しいバージョンでは動かないのでバージョンを指定してインストールします。

.pytorchとは?
keras同様TensorFlowをより便利に使うためのライブラリです。

[source, bash]
----
$ conda install pytorch=0.3.1 -c pytorch
----

続いてREINVENT本体をGitHubからクローンします。

[source, bash]
----
$ cd <path to your working directory>
$ git clone https://github.com/MarcusOlivecrona/REINVENT.git
----

続いて、ChEMBLの110万件くらいのデータセットで予め訓練済みのモデルをダウンロードしてきて元のデータと置き換えます。
このデータはGTX 1080TiGPUマシンを利用して5,6時間かかっていますのでもしトレーニングを自分で行うのであればGPUマシンは必須です。

[source, bash]
----
$ wget https://github.com/Mishima-syk/13/raw/master/generator_handson/data.zip
$ unzip data.zip
$ mv data ./REINVENT/
----

これで準備が整いました。

=== 実例

ここではlink:https://www.drugbank.ca/drugs/DB01261[Januvia]として知られる抗糖尿病薬sitagliptinの類似体を生成するようなモデルを作成してみます。

まずはtanimoto係数をスコアとして類似度の高い構造を生成するようにモデルを訓練します。今回は3000ステップ訓練しますが、大体ちょっと前のMacbook Airで7,8時間かかるので気長に待ちましょう。待てない場合はlink:https://github.com/Mishima-syk/13/tree/master/generator_handson/sitagliptin_agent_3000[ここ]のデータを使ってください。



[source, bash]
----
./main.py --scoring-function tanimoto --scoring-function-kwargs query_structure 'N[C@@H](CC(=O)N1CCn2c(C1)nnc2C(F)(F)F)Cc3cc(F)c(F)cc3F' --num-steps 3000 --sigma 80
----

ここからはjupyter notebookを立ち上げます。

必要なライブラリを読みこみます。sys.path.appendはREINVENTのディレクトリを指定してください。

[source, python]
----
%matplotlib inline
import sys
sys.path.append("[Your REINVENT DIR]")
from rdkit import Chem
from rdkit.Chem import AllChem, DataStructs, Draw
import torch
from model import RNN
from data_structs import Vocabulary
from utils import seq_to_smiles
----

続いて、トレーニングしたモデルから50化合物サンプリングします。

[source, python]
----
voc = Vocabulary(init_from_file="/Users/kzfm/mishima_syk/REINVENT/data/Voc")
Agent = RNN(voc)
Agent.rnn.load_state_dict(torch.load("sitagliptin_agent_3000/Agent.ckpt"))
seqs, agent_likelihood, entropy = Agent.sample(50)
smiles = seq_to_smiles(seqs, voc)
----

実際にどんな構造が生成されたのか見てみましょう。

[source, python]
----
mols = []
for smi in smiles:
    mol = Chem.MolFromSmiles(smi)
    if mol is not None:
        mols.append(mol)

Draw.MolsToGridImage(mols, molsPerRow=3, subImgSize=(500,400))
----

まずまずといったところでしょうか？

image::ch11/ch11_01.png[Sitagliptin analogues]

=== REINVENTについて簡単に説明

link:https://arxiv.org/abs/1704.07555[元論文(Molecular De Novo Design through Deep Reinforcement Learning)]を読みましょう。

（要説明）
